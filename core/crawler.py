import requests
from bs4 import BeautifulSoup
from django.contrib.auth import get_user_model
import json, os, time
from urllib.parse import quote

# Selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

User = get_user_model()

# ==============================================================
# ì œì™¸ í‚¤ì›Œë“œ (ë¹„ IT)
# ==============================================================
EXCLUDE_TITLE_KEYWORDS = [
    "ë°”ë¦¬ìŠ¤íƒ€","ë² ì´ì»¤ë¦¬","ìš”ë¦¬","íŒŒí‹°ì‰","ì£¼ë°©","ì„œë¹™","ë§¤ì¥","ì•ˆë‚´","í™€","ì‹ìŒë£Œ",
    "ìƒë‹´","ì½œì„¼í„°","ì „í™”","ë¦¬ì…‰ì…˜",
]

# ==============================================================
# ëª¨ë“œë³„ í‚¤ì›Œë“œ
# ==============================================================
def get_keyword_by_mode(user, mode):
    if mode == "spec":
        return user.spec_job or ""
    elif mode == "desired":
        return user.desired_job or ""
    elif mode == "both":
        return f"{user.spec_job or ''} {user.desired_job or ''}".strip()
    return ""

# ==============================================================
# Saramin
# ==============================================================
def crawl_saramin(keyword):
    print(f"ğŸ” [Saramin] ê²€ìƒ‰ ì‹œì‘ â†’ {keyword}")

    url = f"https://www.saramin.co.kr/zf_user/search?searchword={keyword}"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code != 200:
            print(f"âš ï¸ ì‚¬ëŒì¸ {r.status_code}")
            return []
    except Exception as e:
        print(f"âš ï¸ ì‚¬ëŒì¸ ì˜ˆì™¸ {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    results = []

    for item in soup.select("div.item_recruit"):
        title_tag = item.select_one("h2.job_tit a")
        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)
        link = "https://www.saramin.co.kr" + title_tag.get("href", "")

        # âŒ ì œì™¸ í•„í„°ë§Œ ìœ ì§€
        if any(x in title for x in EXCLUDE_TITLE_KEYWORDS):
            continue

        # íšŒì‚¬ëª…
        company_tag = item.select_one("div.area_corp strong.corp_name a")
        company = company_tag.get_text(strip=True) if company_tag else ""

        # ë§ˆê°ì¼
        deadline_tag = item.select_one("div.area_job div.job_date span.date")
        deadline = deadline_tag.get_text(strip=True) if deadline_tag else ""

        # ì§€ì—­ / ê²½ë ¥
        cond = item.select("div.area_job div.job_condition span")
        location = ""
        experience = ""

        if len(cond) > 0:
            los = cond[0].select("a")
            location = " ".join(a.get_text(strip=True) for a in los) if los else cond[0].get_text(strip=True)
        if len(cond) > 1:
            experience = cond[1].get_text(strip=True)

        results.append({
            "title": title,
            "company": company,
            "experience": experience,
            "location": location,
            "deadline": deadline,
            "link": link,
            "source": "Saramin"
        })

    print(f"ğŸ‘‰ ì‚¬ëŒì¸ {len(results)}ê±´")
    return results

# ==============================================================
# Selenium Driver
# ==============================================================
def create_driver():
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--window-size=1920,1080")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-blink-features=AutomationControlled")

    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=opts
    )
    driver.implicitly_wait(5)
    return driver

# ==============================================================
# GroupBy ìƒì„¸ (Selenium ì•ˆì •í™”)
# ==============================================================
def fetch_groupby_detail_selenium(driver, url):
    exp = loc = ddl = ""

    try:
        driver.get(url)
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        soup = BeautifulSoup(driver.page_source, "html.parser")

        def extract(label):
            tag = soup.find(string=lambda s: s and label in s)
            if not tag:
                return ""
            parent = tag.find_parent()
            if not parent:
                return ""
            next_div = parent.find_next("div")
            return next_div.get_text(strip=True) if next_div else ""

        exp = extract("ê²½ë ¥")
        loc = extract("ê·¼ë¬´")
        ddl = extract("ë§ˆê°")

    except Exception as e:
        print(f"âš ï¸ GroupBy ìƒì„¸ ì‹¤íŒ¨: {e}")

    return exp, loc, ddl

# ==============================================================
# GroupBy (API + Selenium ìƒì„¸)
# ==============================================================
def crawl_groupby(keyword):
    print(f"ğŸ” [GroupBy API + Selenium ìƒì„¸] ê²€ìƒ‰ â†’ {keyword}")

    base_api = "https://api.groupby.kr/startup-positions/search"
    encoded = quote(keyword)
    limit = 10
    offset = 0
    headers = {
        "User-Agent":"Mozilla/5.0",
        "Accept":"application/json",
        "Referer":"https://groupby.kr/",
        "Origin":"https://groupby.kr",
    }

    results = []
    driver = create_driver()
    page = 0

    while True:
        url = f"{base_api}?limit={limit}&offset={offset}&searchQuery={encoded}"

        try:
            r = requests.get(url, headers=headers, timeout=10)
            if r.status_code != 200:
                print(f"âš ï¸ GroupBy API {r.status_code}")
                break
            data = r.json()
        except Exception as e:
            print(f"âš ï¸ GroupBy API ì˜ˆì™¸: {e}")
            break

        container = data.get("data")
        if not isinstance(container, dict):
            break

        jobs = None
        for k in ["items","results","list","positions"]:
            if isinstance(container.get(k), list):
                jobs = container[k]; print(f"âœ… ë¦¬ìŠ¤íŠ¸ í‚¤ data['{k}']"); break

        if not jobs:
            break

        for j in jobs:
        # âœ… ì‹¤ì œ í•„ë“œ ë§¤í•‘
            title = j.get("name") or ""
            company = (j.get("startup") or {}).get("name", "")
            job_id = j.get("id")

        # âœ… API ê¸°ì¤€ ë°ì´í„°
            experience = j.get("careerType", "")
            loc1 = (j.get("startup") or {}).get("location", "")
            loc2 = j.get("address", "")
            location = loc1 if loc1 else loc2

        # âœ… publishedAt = ë§ˆê° ëŒ€ìš©
            deadline = (j.get("publishedAt") or "")[:10]

            if not job_id:
                    continue

            link = f"https://groupby.kr/positions/{job_id}"

         # âœ… í•„í„°ëŠ” ì œì™¸í‚¤ì›Œë“œë§Œ ìœ ì§€
            if any(x in title for x in EXCLUDE_TITLE_KEYWORDS):
                    continue

            results.append({
                "title": title,
                "company": company,
                "experience": experience,
                "location": location,
                "deadline": deadline,
                "link": link,
                "source": "GroupBy"
            })


            # âŒ keyword í•„í„° ì œê±°, ì œì™¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
            if any(x in title for x in EXCLUDE_TITLE_KEYWORDS):
                continue

            link = f"https://groupby.kr/positions/{job_id}"

        # Selenium ìƒì„¸
            experience, location, deadline = fetch_groupby_detail_selenium(driver, link)

            results.append({
                "title": title,
                "company": company,
                "experience": experience,
                "location": location,
                "deadline": deadline,
                "link": link,
                "source": "GroupBy"
            })

            time.sleep(1)

            offset += limit
            page += 1
            if page >= 5:
                break

            driver.quit()
            print(f"ğŸ‘‰ GroupBy {len(results)}ê±´")
            return results

# ==============================================================
# ì‹¤í–‰
# ==============================================================
def run_weekly_crawl(mode="both"):
    users = User.objects.all()
    base_dir = os.path.join(os.getcwd(), "crawl_results")
    os.makedirs(base_dir, exist_ok=True)

    for user in users:
        keyword = get_keyword_by_mode(user, mode)
        if not keyword:
            print(f"âš ï¸ {user.username}: ê²€ìƒ‰ì–´ ì—†ìŒ")
            continue

        print(f"\n=============== ğŸ¯ {user.username} ({keyword}) ===============")
        print(f"ğŸ”¥ í¬ë¡¤ë§ ëª¨ë“œ: {mode}")

        results = []
        results.extend(crawl_saramin(keyword))
        results.extend(crawl_groupby(keyword))

        # âœ… ì¤‘ë³µ ì œê±°
        results = list({r["link"]: r for r in results}.values())

        path = os.path.join(base_dir, f"results_{mode}_{user.username}.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"[ğŸ’¾] ì €ì¥ ì™„ë£Œ â†’ {path} (ì´ {len(results)}ê±´)")

import requests
from bs4 import BeautifulSoup
from django.contrib.auth import get_user_model
import json
import os

User = get_user_model()

def build_query_for_user(user):
    """ì‚¬ìš©ì ìŠ¤í™ê³¼ í¬ë§ì§ë¬´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ì–´ ìƒì„±"""
    if not user.spec_job and not user.desired_job:
        return None
    return f"{user.spec_job or ''} {user.desired_job or ''} ì±„ìš©ê³µê³ ".strip()


def crawl_jobs(keyword):
    """ì‚¬ëŒì¸ì—ì„œ í•´ë‹¹ í‚¤ì›Œë“œë¡œ ì±„ìš©ê³µê³  í¬ë¡¤ë§"""
    url = f"https://www.saramin.co.kr/zf_user/search?searchword={keyword}"
    response = requests.get(url)
    if response.status_code != 200:
        print(f"[âš ï¸] ìš”ì²­ ì‹¤íŒ¨ ({response.status_code})")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    jobs = []

    for item in soup.select("h2.job_tit a"):
        title = item.text.strip()
        link = "https://www.saramin.co.kr" + item["href"]
        jobs.append({"title": title, "link": link})

    return jobs


def run_weekly_crawl():
    """ì „ì²´ ì‚¬ìš©ìì— ëŒ€í•´ ë§ì¶¤ í¬ë¡¤ë§ ì‹¤í–‰"""
    users = User.objects.all()
    base_dir = os.path.join(os.getcwd(), "crawl_results")
    os.makedirs(base_dir, exist_ok=True)

    for user in users:
        query = build_query_for_user(user)
        if not query:
            print(f"[âš ï¸] {user.username}: ì„ íƒê°’ ì—†ìŒ, ê±´ë„ˆëœ€")
            continue

        results = crawl_jobs(query)
        save_path = os.path.join(base_dir, f"results_{user.username}.json")
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"[ğŸ’¾] {user.username}: {len(results)}ê±´ ì €ì¥ ì™„ë£Œ")

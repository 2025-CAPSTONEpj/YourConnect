import requests, json, os, time, re
from bs4 import BeautifulSoup
from django.contrib.auth import get_user_model
from urllib.parse import quote
from datetime import datetime, timedelta
import glob

User = get_user_model()

# ===============================================================
# íŒŒì¼ ìë™ ì •ë¦¬
# ===============================================================
def cleanup_old_crawl_files(days_to_keep=3):
    """
    crawl_results/ ë””ë ‰í† ë¦¬ì˜ 7ì¼ ì´ìƒ ëœ JSON íŒŒì¼ ìë™ ì‚­ì œ
    
    Args:
        days_to_keep: ìœ ì§€í•  íŒŒì¼ì˜ ìµœì†Œ ì¼ ìˆ˜ (ê¸°ë³¸ê°’: 7ì¼)
    """
    base_dir = os.path.join(os.path.dirname(__file__), '..', 'crawl_results')
    
    if not os.path.exists(base_dir):
        return
    
    cutoff_time = datetime.now() - timedelta(days=days_to_keep)
    deleted_files = []
    
    try:
        json_files = glob.glob(os.path.join(base_dir, '*.json'))
        
        for filepath in json_files:
            # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
            file_mtime = datetime.fromtimestamp(os.path.getmtime(filepath))
            
            # 7ì¼ ì´ìƒ ëœ íŒŒì¼ ì‚­ì œ
            if file_mtime < cutoff_time:
                try:
                    os.remove(filepath)
                    deleted_files.append(os.path.basename(filepath))
                    print(f"ğŸ—‘ï¸ [ì •ë¦¬] ì‚­ì œë¨: {os.path.basename(filepath)}")
                except Exception as e:
                    print(f"âš ï¸ [ì •ë¦¬] ì‚­ì œ ì‹¤íŒ¨ {os.path.basename(filepath)}: {e}")
        
        if deleted_files:
            print(f"âœ… [ì •ë¦¬ ì™„ë£Œ] {len(deleted_files)}ê°œ íŒŒì¼ ì‚­ì œë¨")
        else:
            print(f"âœ… [ì •ë¦¬] ì‚­ì œí•  íŒŒì¼ ì—†ìŒ (7ì¼ ì´ìƒ ëœ íŒŒì¼ ì—†ìŒ)")
            
    except Exception as e:
        print(f"âš ï¸ [ì •ë¦¬] ì˜ˆì™¸ ë°œìƒ: {e}")

# -----------------------
# í‚¤ì›Œë“œ ì •ì±…
# -----------------------
EXCLUDE_TITLE_KEYWORDS = [
    "ë°”ë¦¬ìŠ¤íƒ€","ì¹´í˜","ë² ì´ì»¤ë¦¬","ìš”ë¦¬","íŒŒí‹°ì‰","ì£¼ë°©","ì„œë¹™","ë§¤ì¥","ì•ˆë‚´","í™€","ì‹ìŒë£Œ",
    "ìƒë‹´","ì½œì„¼í„°","ì „í™”","ë¦¬ì…‰ì…˜",
]

# ===============================================================
# ëª¨ë“œë³„ í‚¤ì›Œë“œ
# ===============================================================
def get_keyword_by_mode(user, mode):
    if mode == "spec":
        return user.spec_job or ""
    elif mode == "desired":
        return user.desired_job or ""
    elif mode == "both":
        return f"{user.spec_job or ''} {user.desired_job or ''}".strip()
    return ""

# ===============================================================
# Saramin
# ===============================================================
def crawl_saramin(keyword):
    print(f"ğŸ” [Saramin] ê²€ìƒ‰ ì‹œì‘ â†’ {keyword}")

    url = f"https://www.saramin.co.kr/zf_user/search?searchword={keyword}"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code != 200:
            print("âš ï¸ ì‚¬ëŒì¸ ìš”ì²­ ì‹¤íŒ¨")
            return []
    except Exception as e:
        print("âš ï¸ ì‚¬ëŒì¸ ì˜ˆì™¸:", e)
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    results = []
    debug_count = 0

    for item in soup.select("div.item_recruit"):
        title_tag = item.select_one("h2.job_tit a")
        if not title_tag:
            continue

        title = title_tag.text.strip()
        link = "https://www.saramin.co.kr" + title_tag.get("href", "")

        if any(x in title for x in EXCLUDE_TITLE_KEYWORDS):
            continue

        company_tag = item.select_one("div.area_corp strong.corp_name a")
        company = company_tag.text.strip() if company_tag else ""

        # Saramin ì¡°ê±´ ì •ë³´ ì¶”ì¶œ - span ì¸ë±ì‹± ë°©ì‹
        condition_tags = item.select("div.area_job span")
        location = ""
        experience = ""
        deadline = ""
        
        # DEBUG: ì²« 3ê°œ í•­ëª©ì˜ span êµ¬ì¡°ë¥¼ í™•ì¸
        if debug_count < 3:
            print(f"  [ë””ë²„ê·¸] {debug_count}ë²ˆ í•­ëª© - span ê°œìˆ˜: {len(condition_tags)}")
            for idx, tag in enumerate(condition_tags):
                print(f"    [{idx}]: {tag.text.strip()[:50]}")
            debug_count += 1
        
        # span êµ¬ì¡° ë¶„ì„:
        # [0]: title (ìŠ¤í‚µ)
        # [1]: ? (ë³´í†µ ë¹„ì–´ìˆìŒ)
        # [2]: deadline
        # [3]: ì§€ì› ë°©ì‹/ê¸°íƒ€
        # [4]: ì§€ì—­ëª…
        # [5]: ê²½ë ¥
        # [6]: í•™ë ¥
        # [7]: ê³ ìš©í˜•íƒœ
        # [8]: ìˆ˜ì •ì¼
        
        if len(condition_tags) > 4:
            # ì§€ì—­: index 4
            loc_text = condition_tags[4].text.strip()
            if loc_text:
                location = loc_text
        
        if len(condition_tags) > 5:
            # ê²½ë ¥: index 5
            exp_text = condition_tags[5].text.strip()
            if exp_text and 'ì¡¸' not in exp_text:  # í•™ë ¥ ì œì™¸
                experience = exp_text
        
        if len(condition_tags) > 2:
            # ë§ˆê°ì¼: index 2
            dead_text = condition_tags[2].text.strip()
            if dead_text:
                deadline = dead_text

        # experienceê°€ ë¹„ì–´ìˆìœ¼ë©´ ë‹¤ë¥¸ condition_tagsì—ì„œ ì°¾ê¸°
        if not experience:
            for i, tag in enumerate(condition_tags):
                tag_text = tag.text.strip()
                if 'ë…„' in tag_text and 'ì¡¸' not in tag_text:
                    experience = tag_text
                    break
        
        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if not experience:
            experience = "ë¬´ê´€"

        results.append({
            "title": title,
            "company": company,
            "experience": experience,
            "location": location,
            "deadline": deadline,
            "link": link,
            "source": "Saramin"
        })

    print(f"ğŸ‘‰ ì‚¬ëŒì¸ {len(results)}ê±´")
    return results

# ===============================================================
# GroupBy API (Selenium ì™„ì „ ì œê±°)
# ===============================================================
def crawl_groupby(keyword):
    print(f"ğŸ” [GroupBy API] ê²€ìƒ‰ â†’ {keyword}")

    base_url = "https://api.groupby.kr/startup-positions/search"
    encoded = quote(keyword)
    limit = 10
    offset = 0

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json",
        "Referer": "https://groupby.kr/",
        "Origin": "https://groupby.kr"
    }

    results = []
    page = 1

    while True:
        url = f"{base_url}?limit={limit}&offset={offset}&searchQuery={encoded}"

        try:
            r = requests.get(url, headers=headers, timeout=10)
            if r.status_code != 200:
                print("âš ï¸ GroupBy API ì‹¤íŒ¨:", r.status_code)
                break

            data = r.json()
            print(f"ğŸ“Š GroupBy ì‘ë‹µ ë°ì´í„°: {data}")
        except Exception as e:
            print("âš ï¸ GroupBy ì˜ˆì™¸:", e)
            import traceback
            print(traceback.format_exc())
            break

        container = data.get("data")
        print(f"ğŸ“Š container íƒ€ì…: {type(container)}, ê°’: {container}")
        
        if not isinstance(container, dict):
            print(f"âš ï¸ containerê°€ dictê°€ ì•„ë‹˜: {type(container)}")
            break

        jobs = container.get("items")
        print(f"ğŸ“Š jobs íƒ€ì…: {type(jobs)}, ê¸¸ì´: {len(jobs) if isinstance(jobs, list) else 'N/A'}")
        
        if not isinstance(jobs, list) or not jobs:
            print(f"âš ï¸ jobsê°€ listê°€ ì•„ë‹ˆê±°ë‚˜ ë¹„ì–´ìˆìŒ")
            break

        print("âœ… ë¦¬ìŠ¤íŠ¸ í‚¤ data['items']")

        for j in jobs:
            title = j.get("name") or ""
            startup = j.get("startup", {})

            company = startup.get("name", "")
            location = startup.get("location", "")
            address = startup.get("address", "")

            exp = j.get("experienceRange") or {}
            min_exp = exp.get("min")
            max_exp = exp.get("max")

            experience = ""
            if min_exp is not None and max_exp is not None:
                experience = f"{min_exp}~{max_exp}ë…„"
            elif min_exp is not None:
                experience = f"{min_exp}ë…„ ì´ìƒ"
            elif max_exp is not None:
                experience = f"{max_exp}ë…„ ì´í•˜"

            deadline = j.get("publishedAt")
            
            # GroupBy ë‚ ì§œë¥¼ Saramin í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (~MM.DD(ìš”ì¼))
            if deadline:
                try:
                    from datetime import datetime
                    # ISO format íŒŒì‹±
                    dt = datetime.fromisoformat(deadline.replace('Z', '+00:00'))
                    # "~12.09(ìˆ˜)" í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    day_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
                    day_name = day_names[dt.weekday()]
                    deadline = dt.strftime(f"~%m.%d({day_name})")
                except:
                    deadline = ""
            
            job_id = j.get("id")
            link = f"https://groupby.kr/positions/{job_id}"

            results.append({
                "title": title,
                "company": company,
                "experience": experience,
                "location": location or address,
                "deadline": deadline,
                "link": link,
                "source": "GroupBy"
            })

        offset += limit
        page += 1
        if page > 5:
            break

        time.sleep(1)

    print(f"ğŸ‘‰ GroupBy {len(results)}ê±´")
    return results

# ===============================================================
# ì§€ì—­ í•„í„°ë§ í•¨ìˆ˜
# ===============================================================
def _filter_by_career(results, career):
    """
    ê²½ë ¥ í•„í„°ë§: experience í•„ë“œì—ì„œ ìµœì†Œ ê²½ë ¥ì„ í™•ì¸í•˜ì—¬ í•„í„°ë§
    career í¬ë§·: "1ë…„~3ë…„", "3ë…„~5ë…„", "5ë…„ ì´ìƒ" ë“±
    """
    if not career:
        return results
    
    # career ë¬¸ìì—´ì—ì„œ ìµœì†Œ/ìµœëŒ€ ê²½ë ¥ ì¶”ì¶œ
    career_min = 0
    career_max = 100  # ê¸°ë³¸ê°’: ìƒí•œ ì—†ìŒ
    try:
        if "ì´ìƒ" in career:
            # "5ë…„ ì´ìƒ" â†’ 5 ~ 100
            career_min = int(career.replace("ë…„", "").replace(" ì´ìƒ", "").strip())
            career_max = 100
        elif "~" in career:
            # "1ë…„~3ë…„" â†’ 1 ~ 3
            parts = career.split("~")
            career_min = int(parts[0].replace("ë…„", "").strip())
            career_max = int(parts[1].replace("ë…„", "").strip())
    except:
        return results
    
    filtered = []
    for job in results:
        experience = job.get("experience", "")
        
        # experienceê°€ ë¹„ì–´ìˆìœ¼ë©´ ì œì™¸ (ê²½ë ¥ ì •ë³´ê°€ ì—†ìŒ)
        if not experience:
            continue
        
        # "ë¬´ê´€"ì¸ ê²½ìš° í•­ìƒ í¬í•¨ (ëª¨ë“  ê²½ë ¥ ìˆ˜ì¤€ì— ì í•©)
        if "ë¬´ê´€" in experience:
            filtered.append(job)
            continue
        
        try:
            # experienceì—ì„œ ìµœì†Œ ê²½ë ¥ ì¶”ì¶œ
            exp_min = 0
            exp_max = 100  # ê¸°ë³¸ê°’: ìƒí•œ ì—†ìŒ
            
            # "ê²½ë ¥ 5" í˜•ì‹ ì²˜ë¦¬
            if "ê²½ë ¥" in experience:
                exp_clean = experience.replace("ê²½ë ¥", "").strip()
                # "5~10ë…„" í˜•ì‹ì—ì„œ ì²« ë²ˆì§¸ ìˆ«ìë§Œ ì¶”ì¶œ
                if "~" in exp_clean:
                    parts = exp_clean.split("~")
                    exp_min = int(parts[0].replace("ë…„", "").strip())
                    exp_max = int(parts[1].replace("ë…„", "").strip())
                else:
                    # "5ë…„" í˜•ì‹ì—ì„œ ìˆ«ì ì¶”ì¶œ
                    exp_min = int(exp_clean.replace("ë…„", "").strip())
                    exp_max = exp_min
            elif "ì´ìƒ" in experience:
                # "5ë…„ ì´ìƒ" â†’ 5 ~ 100
                exp_min = int(experience.replace("ë…„", "").replace(" ì´ìƒ", "").strip())
                exp_max = 100
            elif "~" in experience:
                # "1ë…„~3ë…„" â†’ 1 ~ 3
                parts = experience.split("~")
                exp_min = int(parts[0].replace("ë…„", "").strip())
                exp_max = int(parts[1].replace("ë…„", "").strip())
            else:
                # ë‹¨ì¼ ìˆ«ì "3ë…„" â†’ 3
                exp_min = int(experience.replace("ë…„", "").strip())
                exp_max = exp_min
            
            # ê³µê³ ì˜ ìµœì†Œ ê²½ë ¥ì´ ì‚¬ìš©ì ë²”ìœ„ ë‚´ì— ìˆëŠ” ê²½ìš°ë§Œ í¬í•¨
            # exp_min >= career_minì´ê³  exp_min <= career_maxì—¬ì•¼ í•¨
            # (ê³µê³ ê°€ ìš”êµ¬í•˜ëŠ” ìµœì†Œê²½ë ¥ì´ ì‚¬ìš©ìê°€ ê°€ì§„ ë²”ìœ„ ë‚´ì— ìˆì–´ì•¼ í•¨)
            if career_min <= exp_min <= career_max:
                filtered.append(job)
        except ValueError:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ê³µê³ ëŠ” ì œì™¸
            continue
    
    print(f"[ê²½ë ¥ í•„í„°ë§] {career} -> {len(results)}ê±´ ì¤‘ {len(filtered)}ê±´ ì„ íƒ")
    return filtered


def _filter_by_region(results, regions):
    """
    ì§€ì—­ í•„í„°ë§: location í•„ë“œì—ë§Œ ì§€ì—­ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    - regions: ë‹¨ì¼ ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: "ì„œìš¸" ë˜ëŠ” ["ì„œìš¸", "ê²½ê¸°"])
    """
    # ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if isinstance(regions, str):
        if not regions:
            return results
        regions = [regions]
    elif not regions:
        return results
    
    # ì§€ì—­ ì •ë³´ ë§¤í•‘
    region_keywords = {
        "ì„œìš¸": ["ì„œìš¸"],
        "ê²½ê¸°": ["ê²½ê¸°"],
        "ì¸ì²œ": ["ì¸ì²œ"],
        "ëŒ€ì „": ["ëŒ€ì „"],
        "ì„¸ì¢…": ["ì„¸ì¢…"],
        "ì¶©ë‚¨": ["ì¶©ë‚¨"],
        "ì¶©ë¶": ["ì¶©ë¶"],
        "ê´‘ì£¼": ["ê´‘ì£¼"],
        "ì „ë‚¨": ["ì „ë‚¨"],
        "ì „ë¶": ["ì „ë¶"],
        "ëŒ€êµ¬": ["ëŒ€êµ¬"],
        "ê²½ë¶": ["ê²½ë¶"],
        "ë¶€ì‚°": ["ë¶€ì‚°"],
        "ìš¸ì‚°": ["ìš¸ì‚°"],
        "ê²½ë‚¨": ["ê²½ë‚¨"],
        "ê°•ì›": ["ê°•ì›"],
        "ì œì£¼": ["ì œì£¼"]
    }
    
    # ëª¨ë“  ì§€ì—­ì˜ í‚¤ì›Œë“œë¥¼ ì¡°í•©
    all_keywords = []
    for region in regions:
        keywords = region_keywords.get(region, [region])
        all_keywords.extend(keywords)
    
    filtered = []
    excluded_saramin = []  # DEBUG
    
    # ë¬´ì‹œí•  í‚¤ì›Œë“œ (locationì— ì´ëŸ° ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ì§€ì—­ì´ ì•„ë‹˜)
    exclude_keywords = ["ì…ì‚¬ì§€ì›", "ì±„ìš©", "ê³µê³ ", "ì§€ì›", "ë¬¸ì˜", "ì—°ë½", "ì‹ ì²­", "ë³´ê¸°", "ìì„¸íˆ"]
    
    for job in results:
        location = job.get("location", "").strip()
        is_saramin = "saramin" in job.get("source", "").lower()
        
        # locationì´ ë¹„ì–´ìˆìœ¼ë©´ ì œì™¸
        if not location:
            if is_saramin:
                excluded_saramin.append(f"[locationì´ ë¹„ì–´ìˆìŒ] {job.get('title', '')[:50]}")
            continue
        
        # ì œì™¸ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì œì™¸
        excluded_kw = None
        for exc in exclude_keywords:
            if exc in location:
                excluded_kw = exc
                break
        
        if excluded_kw:
            if is_saramin:
                excluded_saramin.append(f"[ì œì™¸í‚¤ì›Œë“œ: {excluded_kw}] {location}")
            continue
        
        # ìš”ì²­ëœ ì§€ì—­ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ í†µê³¼ (OR ë¡œì§)
        if any(kw in location for kw in all_keywords):
            filtered.append(job)
        else:
            if is_saramin:
                excluded_saramin.append(f"[ì§€ì—­ë¯¸ë§¤ì¹­] {location} (ì°¾ëŠ” ì§€ì—­: {all_keywords})")
    
    regions_str = ", ".join(regions) if isinstance(regions, list) else regions
    print(f"ğŸ” [ì§€ì—­ í•„í„°ë§] {regions_str} â†’ {len(results)}ê±´ ì¤‘ {len(filtered)}ê±´ ì„ íƒ")
    if excluded_saramin:
        print(f"  âš ï¸ ì œì™¸ëœ Saramin:")
        for msg in excluded_saramin[:10]:
            print(f"    {msg}")
    return filtered


# ===============================================================
# í•„í„° ì¡°ê±´ ê¸°ë°˜ í¬ë¡¤ë§ (í—¤ë“œí—ŒíŒ…ìš©)
# ===============================================================
def crawl_with_filters(duty="", subDuties=None, position="", career="", region=""):
    """
    í•„í„° ì¡°ê±´ì— ë§ê²Œ í¬ë¡¤ë§ ìˆ˜í–‰
    duty: ëŒ€ë¶„ë¥˜ ì§ë¬´ (ê°œë°œ, ë°ì´í„°, ê¸°íš ë“±)
    subDuties: ì„¸ë¶€ ì§ë¬´ ë¦¬ìŠ¤íŠ¸ (["FE", "BE"] ë“±)
    position: ì§ê¸‰ (í˜„ì¬ ì‚¬ìš© ì•ˆ í•¨)
    career: ê²½ë ¥ (1ë…„~3ë…„, 3ë…„~5ë…„ ë“±)
    region: ì§€ì—­ (ì„œìš¸, ê²½ê¸°, ë¶€ì‚° ë“±)
    """
    if subDuties is None:
        subDuties = []
    
    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¡°í•© ìƒì„±
    # ì„¸ë¶€ ì§ë¬´ê°€ ìˆìœ¼ë©´ dutyì™€ í•¨ê»˜ í¬í•¨ (ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼)
    keywords = []
    
    # dutyì™€ subDuties ëª¨ë‘ í¬í•¨í•˜ë˜, ë¹ˆ ë¬¸ìì—´ ì œê±°
    if duty:
        keywords.append(duty)
    
    if subDuties:
        keywords.extend([s for s in subDuties if s])
    
    if region and region != 'ì„œìš¸':
        keywords.append(region)
    if career:
        keywords.append(career)
    
    search_keyword = " ".join(filter(None, keywords))
    
    # ë§Œì•½ ê²€ìƒ‰ì–´ê°€ ë¹„ì–´ìˆìœ¼ë©´ dutyë§Œ ì‚¬ìš©
    if not search_keyword and duty:
        search_keyword = duty
    
    results = []
    
    # Saramin í¬ë¡¤ë§ (ì—ëŸ¬ ì²˜ë¦¬)
    try:
        saramin_results = crawl_saramin(search_keyword)
        print(f"ğŸ“Š [Saramin ì›ë³¸] {len(saramin_results)}ê±´")
        for i, job in enumerate(saramin_results[:5]):  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
            print(f"  {i+1}. {job['title'][:30]} | {job.get('location', 'N/A')} | {job.get('experience', 'N/A')}")
        results.extend(saramin_results)
        print(f"âœ… Saramin í¬ë¡¤ë§ ì„±ê³µ: {len(saramin_results)}ê±´")
    except Exception as e:
        import traceback
        print(f"âš ï¸ Saramin í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        print(f"âš ï¸ ìƒì„¸: {traceback.format_exc()}")
    
    # GroupBy í¬ë¡¤ë§ (ì—ëŸ¬ ì²˜ë¦¬)
    try:
        groupby_results = crawl_groupby(search_keyword)
        results.extend(groupby_results)
        print(f"âœ… GroupBy í¬ë¡¤ë§ ì„±ê³µ: {len(groupby_results)}ê±´")
    except Exception as e:
        print(f"âš ï¸ GroupBy í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
    results = list({r["link"]: r for r in results}.values())
    print(f"âœ… [ì¤‘ë³µ ì œê±° í›„] {len(results)}ê±´")
    
    # ê²½ë ¥ í•„í„°ë§ (ìš”ì²­í•œ ê²½ë ¥ ë²”ìœ„ì—ë§Œ í¬í•¨)
    if career:
        results = _filter_by_career(results, career)
    
    # ì§€ì—­ í•„í„°ë§ (ìš”ì²­í•œ ì§€ì—­ë§Œ í¬í•¨)
    # ë””ë²„ê·¸: Saramin ê²°ê³¼ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì¼ë‹¨ í•„í„°ë§ ì•ì˜ ê²°ê³¼ë¥¼ ì¶œë ¥
    print(f"ğŸ” [ì§€ì—­ í•„í„°ë§ ì „] ì´ {len(results)}ê±´")
    for job in results[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
        print(f"  - {job['source']}: {job.get('location', 'N/A')} | {job['title'][:30]}")
    
    if region:
        results = _filter_by_region(results, region)
    
    # ê²°ê³¼ ì •ë ¬ (ìµœì‹  ê³µê³  ìš°ì„  - deadline ê¸°ì¤€)
    results.sort(key=lambda x: x.get("deadline") or "", reverse=True)

    print(f"âœ… [í•„í„° í¬ë¡¤ë§] ì™„ë£Œ â†’ ì´ {len(results)}ê±´")
    return results


# ===============================================================
# ì´ë©”ì¼ ìƒì„± í•¨ìˆ˜
# ===============================================================
def generate_email_html(user, crawl_results):
    """
    í¬ë¡¤ë§ ê²°ê³¼ë¥¼ HTML ì´ë©”ì¼ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    
    Args:
        user: User ê°ì²´
        crawl_results: í¬ë¡¤ë§ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        HTML ë¬¸ìì—´
    """
    if not crawl_results:
        return f"""
        <html>
            <body style="font-family: Arial, sans-serif; color: #333;">
                <h2>ğŸ¯ {user.username}ë‹˜ì˜ ì±„ìš© ê³µê³  ê²€ìƒ‰ ê²°ê³¼</h2>
                <p>ê²€ìƒ‰ì–´: {user.spec_job}</p>
                <p style="color: #999;">ğŸ“­ í˜„ì¬ ë§¤ì¹­ë˜ëŠ” ì±„ìš© ê³µê³ ê°€ ì—†ìŠµë‹ˆë‹¤.</p>
            </body>
        </html>
        """
    
    job_html = ""
    for idx, job in enumerate(crawl_results[:20], 1):  # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
        job_html += f"""
        <tr>
            <td style="padding: 12px; border-bottom: 1px solid #ddd;">
                <strong>{idx}. {job.get('title', 'N/A')[:50]}</strong><br>
                <small style="color: #666;">
                    ğŸ¢ {job.get('company', 'N/A')} | ğŸ“ {job.get('location', 'N/A')} | â° {job.get('deadline', 'N/A')}<br>
                    ì¶œì²˜: {job.get('source', 'Unknown')}
                </small>
                <br><a href="{job.get('link', '#')}" style="color: #0066cc; text-decoration: none;">ğŸ‘‰ ìì„¸íˆ ë³´ê¸°</a>
            </td>
        </tr>
        """
    
    html_content = f"""
    <html>
        <body style="font-family: Arial, sans-serif; color: #333; background-color: #f5f5f5; padding: 20px;">
            <div style="max-width: 600px; margin: 0 auto; background-color: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
                <h2 style="color: #0066cc;">ğŸ¯ {user.username}ë‹˜ì˜ ì±„ìš© ê³µê³  ê²€ìƒ‰ ê²°ê³¼</h2>
                <p style="color: #666;">ê²€ìƒ‰ì–´: <strong>{user.spec_job}</strong> | ì´ <strong>{len(crawl_results)}</strong>ê°œ ê³µê³ </p>
                <hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
                
                <table style="width: 100%; border-collapse: collapse;">
                    {job_html}
                </table>
                
                <hr style="border: none; border-top: 1px solid #ddd; margin: 20px 0;">
                <p style="color: #999; font-size: 12px;">
                    ì´ ì´ë©”ì¼ì€ ìë™ìœ¼ë¡œ ë°œì†¡ë˜ì—ˆìŠµë‹ˆë‹¤. ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 9ì‹œì— ë°œì†¡ë©ë‹ˆë‹¤.<br>
                    ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰ ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                </p>
            </div>
        </body>
    </html>
    """
    
    return html_content


# ===============================================================
# ì‹¤í–‰ ì—”ì§„
# ===============================================================
def run_weekly_crawl(mode="desired"):
    users = User.objects.all()
    base_dir = os.path.join(os.getcwd(), "crawl_results")
    os.makedirs(base_dir, exist_ok=True)

    for user in users:
        keyword = get_keyword_by_mode(user, mode)
        if not keyword:
            print(f"âš ï¸ {user.username} : ê²€ìƒ‰ì–´ ì—†ìŒ")
            continue

        print(f"\n=============== ğŸ¯ {user.username} ({keyword}) ===============")
        print(f"ğŸ”¥ í¬ë¡¤ë§ ëª¨ë“œ: {mode}")

        results = []
        results.extend(crawl_saramin(keyword))
        results.extend(crawl_groupby(keyword))

        results = list({r["link"]: r for r in results}.values())

        path = os.path.join(base_dir, f"results_{mode}_{user.username}.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"[ğŸ’¾] ì €ì¥ ì™„ë£Œ â†’ {path} | ì´ {len(results)}ê±´")

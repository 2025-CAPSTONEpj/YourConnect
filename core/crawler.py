import requests, json, os, time, re
from bs4 import BeautifulSoup
from django.contrib.auth import get_user_model
from urllib.parse import quote

User = get_user_model()

# -----------------------
# í‚¤ì›Œë“œ ì •ì±…
# -----------------------
EXCLUDE_TITLE_KEYWORDS = [
    "ë°”ë¦¬ìŠ¤íƒ€","ì¹´í˜","ë² ì´ì»¤ë¦¬","ìš”ë¦¬","íŒŒí‹°ì‰","ì£¼ë°©","ì„œë¹™","ë§¤ì¥","ì•ˆë‚´","í™€","ì‹ìŒë£Œ",
    "ìƒë‹´","ì½œì„¼í„°","ì „í™”","ë¦¬ì…‰ì…˜",
]

# ===============================================================
# ëª¨ë“œë³„ í‚¤ì›Œë“œ
# ===============================================================
def get_keyword_by_mode(user, mode):
    if mode == "spec":
        return user.spec_job or ""
    elif mode == "desired":
        return user.desired_job or ""
    elif mode == "both":
        return f"{user.spec_job or ''} {user.desired_job or ''}".strip()
    return ""

# ===============================================================
# Saramin
# ===============================================================
def crawl_saramin(keyword):
    print(f"ğŸ” [Saramin] ê²€ìƒ‰ ì‹œì‘ â†’ {keyword}")

    url = f"https://www.saramin.co.kr/zf_user/search?searchword={keyword}"
    headers = {"User-Agent": "Mozilla/5.0"}

    try:
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code != 200:
            print("âš ï¸ ì‚¬ëŒì¸ ìš”ì²­ ì‹¤íŒ¨")
            return []
    except Exception as e:
        print("âš ï¸ ì‚¬ëŒì¸ ì˜ˆì™¸:", e)
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    results = []

    for item in soup.select("div.item_recruit"):
        title_tag = item.select_one("h2.job_tit a")
        if not title_tag:
            continue

        title = title_tag.text.strip()
        link = "https://www.saramin.co.kr" + title_tag.get("href", "")

        if any(x in title for x in EXCLUDE_TITLE_KEYWORDS):
            continue

        company_tag = item.select_one("div.area_corp strong.corp_name a")
        company = company_tag.text.strip() if company_tag else ""

        # Saramin ì¡°ê±´ ì •ë³´ ì¶”ì¶œ - span ì¸ë±ì‹± ë°©ì‹
        condition_tags = item.select("div.area_job span")
        location = ""
        experience = ""
        deadline = ""
        
        # span êµ¬ì¡° ë¶„ì„:
        # [0]: title (ìŠ¤í‚µ)
        # [1]: ? (ë³´í†µ ë¹„ì–´ìˆìŒ)
        # [2]: deadline
        # [3]: ì§€ì› ë°©ì‹/ê¸°íƒ€
        # [4]: ì§€ì—­ëª…
        # [5]: ê²½ë ¥
        # [6]: í•™ë ¥
        # [7]: ê³ ìš©í˜•íƒœ
        # [8]: ìˆ˜ì •ì¼
        
        if len(condition_tags) > 4:
            # ì§€ì—­: index 4
            loc_text = condition_tags[4].text.strip()
            if loc_text:
                location = loc_text
        
        if len(condition_tags) > 5:
            # ê²½ë ¥: index 5
            exp_text = condition_tags[5].text.strip()
            if exp_text and 'ì¡¸' not in exp_text:  # í•™ë ¥ ì œì™¸
                experience = exp_text
        
        if len(condition_tags) > 2:
            # ë§ˆê°ì¼: index 2
            dead_text = condition_tags[2].text.strip()
            if dead_text:
                deadline = dead_text

        results.append({
            "title": title,
            "company": company,
            "experience": experience,
            "location": location,
            "deadline": deadline,
            "link": link,
            "source": "Saramin"
        })

    print(f"ğŸ‘‰ ì‚¬ëŒì¸ {len(results)}ê±´")
    return results

# ===============================================================
# GroupBy API (Selenium ì™„ì „ ì œê±°)
# ===============================================================
def crawl_groupby(keyword):
    print(f"ğŸ” [GroupBy API] ê²€ìƒ‰ â†’ {keyword}")

    base_url = "https://api.groupby.kr/startup-positions/search"
    encoded = quote(keyword)
    limit = 10
    offset = 0

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json",
        "Referer": "https://groupby.kr/",
        "Origin": "https://groupby.kr"
    }

    results = []
    page = 1

    while True:
        url = f"{base_url}?limit={limit}&offset={offset}&searchQuery={encoded}"

        try:
            r = requests.get(url, headers=headers, timeout=10)
            if r.status_code != 200:
                print("âš ï¸ GroupBy API ì‹¤íŒ¨:", r.status_code)
                break

            data = r.json()
        except Exception as e:
            print("âš ï¸ GroupBy ì˜ˆì™¸:", e)
            break

        container = data.get("data")
        if not isinstance(container, dict):
            break

        jobs = container.get("items")
        if not isinstance(jobs, list) or not jobs:
            break

        print("âœ… ë¦¬ìŠ¤íŠ¸ í‚¤ data['items']")

        for j in jobs:
            title = j.get("name") or ""
            startup = j.get("startup", {})

            company = startup.get("name", "")
            location = startup.get("location", "")
            address = startup.get("address", "")

            exp = j.get("experienceRange") or {}
            min_exp = exp.get("min")
            max_exp = exp.get("max")

            experience = ""
            if min_exp is not None and max_exp is not None:
                experience = f"{min_exp}~{max_exp}ë…„"
            elif min_exp is not None:
                experience = f"{min_exp}ë…„ ì´ìƒ"
            elif max_exp is not None:
                experience = f"{max_exp}ë…„ ì´í•˜"

            deadline = j.get("publishedAt")
            job_id = j.get("id")
            link = f"https://groupby.kr/positions/{job_id}"

            results.append({
                "title": title,
                "company": company,
                "experience": experience,
                "location": location or address,
                "deadline": deadline,
                "link": link,
                "source": "GroupBy"
            })

        offset += limit
        page += 1
        if page > 5:
            break

        time.sleep(1)

    print(f"ğŸ‘‰ GroupBy {len(results)}ê±´")
    return results

# ===============================================================
# ì§€ì—­ í•„í„°ë§ í•¨ìˆ˜
# ===============================================================
def _filter_by_region(results, region):
    """
    ì§€ì—­ í•„í„°ë§: location í•„ë“œì— ì§€ì—­ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    """
    if not region:
        return results
    
    # ì§€ì—­ ì •ë³´ ë§¤í•‘
    region_keywords = {
        "ì„œìš¸": ["ì„œìš¸"],
        "ê²½ê¸°": ["ê²½ê¸°"],
        "ì¸ì²œ": ["ì¸ì²œ"],
        "ëŒ€ì „": ["ëŒ€ì „"],
        "ì„¸ì¢…": ["ì„¸ì¢…"],
        "ì¶©ë‚¨": ["ì¶©ë‚¨"],
        "ì¶©ë¶": ["ì¶©ë¶"],
        "ê´‘ì£¼": ["ê´‘ì£¼"],
        "ì „ë‚¨": ["ì „ë‚¨"],
        "ì „ë¶": ["ì „ë¶"],
        "ëŒ€êµ¬": ["ëŒ€êµ¬"],
        "ê²½ë¶": ["ê²½ë¶"],
        "ë¶€ì‚°": ["ë¶€ì‚°"],
        "ìš¸ì‚°": ["ìš¸ì‚°"],
        "ê²½ë‚¨": ["ê²½ë‚¨"],
        "ê°•ì›": ["ê°•ì›"],
        "ì œì£¼": ["ì œì£¼"]
    }
    
    keywords = region_keywords.get(region, [region])
    filtered = []
    
    for job in results:
        location = job.get("location", "")
        experience = job.get("experience", "")
        
        # locationì´ë‚˜ experience í•„ë“œì— ì§€ì—­ëª…ì´ ìˆìœ¼ë©´ í¬í•¨
        # (Saraminì—ì„œ locationê³¼ experience í•„ë“œ ì„ì„ ë°©ì§€)
        if any(kw in location for kw in keywords) or any(kw in experience for kw in keywords):
            # ê²½í—˜ì´ ì‹¤ì œë¡œ ì§€ì—­ì´ ì•„ë‹Œì§€ í™•ì¸ (ì˜ˆ: "ê²½ê¸° ì‹œí¥ì‹œ"ëŠ” locationì´ì§€ë§Œ experience í•„ë“œì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆìŒ)
            # experience í•„ë“œê°€ ì‹¤ì œë¡œ ê²½ë ¥ ê´€ë ¨ ì •ë³´ê°€ ì•„ë‹ˆë©´ location ì •ë³´ë¡œ ê°„ì£¼
            filtered.append(job)
        elif not experience or (not any(x in experience for x in ["ë…„", "ê°œì›”", "ì‹ ì…", "ê²½ë ¥", "ë¬´ê´€"])):
            # experienceê°€ ë¹„ì–´ìˆê±°ë‚˜ ê²½ë ¥ ì •ë³´ê°€ ì•„ë‹Œ ê²½ìš°, ì§€ì—­ í‚¤ì›Œë“œê°€ locationì—ë§Œ ìˆì–´ë„ í¬í•¨
            if any(kw in location for kw in keywords):
                filtered.append(job)
    
    print(f"ğŸ” [ì§€ì—­ í•„í„°ë§] {region} â†’ {len(results)}ê±´ ì¤‘ {len(filtered)}ê±´ ì„ íƒ")
    return filtered


# ===============================================================
# í•„í„° ì¡°ê±´ ê¸°ë°˜ í¬ë¡¤ë§ (í—¤ë“œí—ŒíŒ…ìš©)
# ===============================================================
def crawl_with_filters(duty="", subDuties=None, position="", career="", region=""):
    """
    í•„í„° ì¡°ê±´ì— ë§ê²Œ í¬ë¡¤ë§ ìˆ˜í–‰
    duty: ëŒ€ë¶„ë¥˜ ì§ë¬´ (ê°œë°œ, ë°ì´í„°, ê¸°íš ë“±)
    subDuties: ì„¸ë¶€ ì§ë¬´ ë¦¬ìŠ¤íŠ¸ (["FE", "BE"] ë“±)
    position: ì§ê¸‰ (í˜„ì¬ ì‚¬ìš© ì•ˆ í•¨)
    career: ê²½ë ¥ (1ë…„~3ë…„, 3ë…„~5ë…„ ë“±)
    region: ì§€ì—­ (ì„œìš¸, ê²½ê¸°, ë¶€ì‚° ë“±)
    """
    if subDuties is None:
        subDuties = []
    
    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¡°í•© ìƒì„±
    # ì—¬ëŸ¬ ê°œì˜ ì„¸ë¶€ ì§ë¬´ê°€ ìˆìœ¼ë©´ ëª¨ë‘ í¬í•¨
    if subDuties:
        keywords = subDuties
    else:
        keywords = [duty]
    
    if region and region != 'ì„œìš¸':
        keywords.append(region)
    if career:
        keywords.append(career)
    
    search_keyword = " ".join(filter(None, keywords))
    
    print(f"ğŸ” [í•„í„° í¬ë¡¤ë§] ê²€ìƒ‰ í‚¤ì›Œë“œ â†’ {search_keyword}")
    print(f"   ì§ë¬´: {duty} | ì„¸ë¶€: {subDuties} | ê²½ë ¥: {career} | ì§€ì—­: {region}")

    results = []
    
    # Saramin í¬ë¡¤ë§ (ì—ëŸ¬ ì²˜ë¦¬)
    try:
        saramin_results = crawl_saramin(search_keyword)
        results.extend(saramin_results)
        print(f"âœ… Saramin í¬ë¡¤ë§ ì„±ê³µ: {len(saramin_results)}ê±´")
    except Exception as e:
        print(f"âš ï¸ Saramin í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    # GroupBy í¬ë¡¤ë§ (ì—ëŸ¬ ì²˜ë¦¬)
    try:
        groupby_results = crawl_groupby(search_keyword)
        results.extend(groupby_results)
        print(f"âœ… GroupBy í¬ë¡¤ë§ ì„±ê³µ: {len(groupby_results)}ê±´")
    except Exception as e:
        print(f"âš ï¸ GroupBy í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

    # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
    results = list({r["link"]: r for r in results}.values())
    
    # ì§€ì—­ í•„í„°ë§ (ìš”ì²­í•œ ì§€ì—­ë§Œ í¬í•¨)
    if region:
        results = _filter_by_region(results, region)
    
    # ê²°ê³¼ ì •ë ¬ (ìµœì‹  ê³µê³  ìš°ì„  - deadline ê¸°ì¤€)
    results.sort(key=lambda x: x.get("deadline", ""), reverse=True)

    print(f"âœ… [í•„í„° í¬ë¡¤ë§] ì™„ë£Œ â†’ ì´ {len(results)}ê±´")
    return results


# ===============================================================
# ì‹¤í–‰ ì—”ì§„
# ===============================================================
def run_weekly_crawl(mode="desired"):
    users = User.objects.all()
    base_dir = os.path.join(os.getcwd(), "crawl_results")
    os.makedirs(base_dir, exist_ok=True)

    for user in users:
        keyword = get_keyword_by_mode(user, mode)
        if not keyword:
            print(f"âš ï¸ {user.username} : ê²€ìƒ‰ì–´ ì—†ìŒ")
            continue

        print(f"\n=============== ğŸ¯ {user.username} ({keyword}) ===============")
        print(f"ğŸ”¥ í¬ë¡¤ë§ ëª¨ë“œ: {mode}")

        results = []
        results.extend(crawl_saramin(keyword))
        results.extend(crawl_groupby(keyword))

        results = list({r["link"]: r for r in results}.values())

        path = os.path.join(base_dir, f"results_{mode}_{user.username}.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"[ğŸ’¾] ì €ì¥ ì™„ë£Œ â†’ {path} | ì´ {len(results)}ê±´")
